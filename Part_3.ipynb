{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Part 3",
      "metadata": {
        "tags": [],
        "cell_id": "00000-0940c6bd-8ca4-4971-bf4a-c05f24e3bfe2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Lecture 3\n\nDeep learning uses representation learning to learn the hyperparameters which handles extraction of features and act as classifier.\n\n<img src=\"/image/CNN_average.png\" height=\"250\" />\n\nA convolution that performs neighbourhood average.\n- Pixels are lost at the border\n    - Solution: add padding\n        - If kernel is even, it has no center, thus uneven padding\n        - Ex. 100x100 px image -> **5x5 filter without padding**\n            - Lose 2 px on all sides.\n                - **784 border pxls lost**.\n\n<img src=\"/image/Kernel_size.png\" height=\"250\" />\n\n**Small kernel**\n- more focus in back\n\n**Big kernel**\n- more focus in front\n\n**Feed forward network vs Convolutional network**\nA feed forward Toeplitz matrix (diagonal-constant matrix) is same as convolution\n- Which is a sparse matrix, localized and shares parameters\n- CNN is thus limited parameter version of FFN\n    - increase receptive field _**linearly**_\n    - A convolution can be written as **matrix multiplication**\n- Less parameters is a good thing because Curse of dimensionality\n\n**Equivariance**:\nIf input shits left, out also does\n- $f()$ is equivariant to $g()$: $f(g(x))$ = $g(f(x))$\n    - so if $f()$ was a concolution and $g()$ a translation, it wouldn't matter which order it was done.\n- Camera position is accidental, objects may appear anywhere\n    - Add prior knowledge (convolution) to deep nets\n        - saves params and compute",
      "metadata": {
        "tags": [],
        "cell_id": "00001-563a9bcd-cb2b-43cd-aca0-9c129e940e9b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<img src=\"/image/Pooling.png\" height=\"150\" />\n\n**Pooling**:\n- Summaries outcome over a region\n- increase receptive field _**multiplicatively**_\n- Is (approximately) invariant to local translations\n- **Feature presence** is _more_ important then feature location\n- Reduce memory usage is advantage of pooling\n- Sub-sampling (pooling) allows to quickly 'see' more of the image.\n\n<img src=\"/image/Stride.png\" height=\"150\" />\n\n**Stride**:\n- kernel will move (stride) pixels per time during a convolution\n- Ex:\n    - two size 3 convolutions leads to 7 pixels **receptive field**.\n\n\n\n<img src=\"/image/Network_with_CNN.png\" height=\"250\" />\n\nBelow we perform two convolutions with subsampling. Shown in image above is an example of such network.\n\n<img src=\"/image/receptive_field_calc.jpg\" height=\"250\" />\n\nAbove, is a calculation of the receptive field for given example, which is 16x16",
      "metadata": {
        "tags": [],
        "cell_id": "00002-9ae8ccf8-ca04-4bea-9b52-0859c8a76146",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Assignment 3: Convolution\n\nA Linear (fully connected) layer treats pixels **far apart and close equally**. A CNN has \n- only a **receptive field**\n    - region hidden neuron can observe. \n- only learns the weights connected to that region.\n- Has shared weights.\n\n<img src=\"/image/Hidden_neuron_receptive_field.png\" height=\"250\" />\n\nProperties:\n- **Stride**: pixels between each receptive field pixel\n- **Padding**: prevent becoming smaller\n- **Kernel**: receptive field size\n- **Pooling**: simplify information CNN output; used to reduce the dimensions of the feature maps, thus reducing nr of parameters to learn & computation.\n    - max pooling: condense information by taking maximum value\n    - average pooling: condense information by taking average value\n    - L2 pooling: condense information by taking square root of sum of squares.\n\nNetwork:\n- training? net.train\n- evaluating? net.eval",
      "metadata": {
        "tags": [],
        "cell_id": "00002-ce72d048-d085-4150-b1ac-8bfce8746db2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<img src=\"/image/shallow_cnn_ex.jpeg\" height=\"250\" />",
      "metadata": {
        "tags": [],
        "cell_id": "00004-f34d5875-7cdf-420b-830a-8a1dd2949425",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-6a780c9b-a176-4d0f-b931-0ee8ebf92442",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "60426d5a",
        "execution_millis": 5,
        "execution_start": 1617986370849,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn as nn\nfrom torchinfo import summary\n\nbatch_size = 16\nwidth_image = 32\nheigh_image = 32\nlayers_image = 3 # Example: 3 -> RGB\ninput_image = (width_image, heigh_image, layers_image)\nwh_1 = 5 # 1st layer: (Width, Height)\nf_1 = 5 # 1st layer: nr of filters\nwh_2 = 3 # 1st layer: (Width, Height)\nf_2 = 2 # 1st layer: nr of filters\nbias=False\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(layers_image, f_1, wh_1, bias=bias)\n        self.conv2 = nn.Conv2d(f_1, f_2, wh_2, bias=bias)\n\n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\nmodel_ouput = summary(\n    Net(), \n    (batch_size, layers_image, heigh_image, width_image),\n    verbose=2,\n    col_width=16,\n    col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"],)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "========================================================================================================\nLayer (type:depth-idx)                   Kernel Shape     Input Shape      Output Shape     Param #\n========================================================================================================\n├─Conv2d: 1-1                            [3, 5, 5, 5]     [16, 3, 32, 32]  [16, 5, 28, 28]  375\n├─Conv2d: 1-2                            [5, 2, 3, 3]     [16, 5, 28, 28]  [16, 2, 26, 26]  90\n========================================================================================================\nTotal params: 465\nTrainable params: 465\nNon-trainable params: 0\nTotal mult-adds (M): 0.35\n========================================================================================================\nInput size (MB): 0.19\nForward/backward pass size (MB): 0.64\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.83\n========================================================================================================\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "832d570c-1889-433b-b8ca-d922d847299a",
    "deepnote_execution_queue": []
  }
}