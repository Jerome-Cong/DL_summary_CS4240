{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00000-0940c6bd-8ca4-4971-bf4a-c05f24e3bfe2",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "# Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-563a9bcd-cb2b-43cd-aca0-9c129e940e9b",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## Lecture 3\n",
        "\n",
        "Deep learning uses representation learning to learn the hyperparameters which handles extraction of features and act as classifier.\n",
        "\n",
        "<img src=\"./image/CNN_average.png\" height=\"250\" />\n",
        "\n",
        "A convolution that performs neighbourhood average.\n",
        "- Pixels are lost at the border\n",
        "    - Solution: add padding\n",
        "        - If kernel is even, it has no center, thus uneven padding\n",
        "        - Ex. 100x100 px image -> **5x5 filter without padding**\n",
        "            - Lose 2 px on all sides.\n",
        "                - **784 border pxls lost**.\n",
        "\n",
        "<img src=\"./image/Kernel_size.png\" height=\"250\" />\n",
        "\n",
        "**Small kernel**\n",
        "- more focus in back\n",
        "\n",
        "**Big kernel**\n",
        "- more focus in front\n",
        "\n",
        "**Spatial pooling**\n",
        "- Images are down-sampled \n",
        "    - faster to compute\n",
        "    - the same kernel can detect larger features in the next layer\n",
        "\n",
        "**Feed forward network vs Convolutional network**\n",
        "A feed forward Toeplitz matrix (diagonal-constant matrix) is same as convolution\n",
        "- Which is a sparse matrix, localized and shares parameters\n",
        "- CNN is thus limited parameter version of FFN\n",
        "    - increase receptive field _**linearly**_\n",
        "    - A convolution can be written as **matrix multiplication**\n",
        "- Less parameters is a good thing because Curse of dimensionality\n",
        "\n",
        "**Equivariance**:\n",
        "If input shits left, out also does\n",
        "- $f()$ is equivariant to $g()$: $f(g(x))$ = $g(f(x))$\n",
        "    - so if $f()$ was a concolution and $g()$ a translation, it wouldn't matter which order it was done.\n",
        "- Camera position is accidental, objects may appear anywhere\n",
        "    - Add prior knowledge (convolution) to deep nets\n",
        "        - saves params and compute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-9ae8ccf8-ca04-4bea-9b52-0859c8a76146",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "<img src=\"./image/Pooling.png\" height=\"150\" />\n",
        "\n",
        "**Pooling**:\n",
        "- Summaries outcome over a region\n",
        "- increase receptive field _**multiplicatively**_\n",
        "- Is (approximately) invariant to local translations\n",
        "- **Feature presence** is _more_ important then feature location\n",
        "- Reduce memory usage is advantage of pooling\n",
        "- Sub-sampling (pooling) allows to quickly 'see' more of the image.\n",
        "\n",
        "<img src=\"./image/Stride.png\" height=\"150\" />\n",
        "\n",
        "**Stride**:\n",
        "- kernel will move (stride) pixels per time during a convolution\n",
        "- Convolution increases receptive field (RF) linearly, pooling increases RF multiplicatively\n",
        "- Ex:\n",
        "    - two size 3 convolutions leads to 7 pixels **receptive field**.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"./image/Network_with_CNN.png\" height=\"250\" />\n",
        "\n",
        "Below we perform two convolutions with subsampling. Shown in image above is an example of such network.\n",
        "\n",
        "<img src=\"./image/receptive_field_calc.jpg\" height=\"250\" />\n",
        "\n",
        "Above, is a calculation of the receptive field for given example, which is 16x16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-ce72d048-d085-4150-b1ac-8bfce8746db2",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## [Assignment 3: Convolution](https://colab.research.google.com/drive/154rVpzuR3MlEUIRIYmYlZ3uy7Jczv0CX)\n",
        "\n",
        "A Linear (fully connected) layer treats pixels **far apart and close equally**. A CNN has \n",
        "- only a **receptive field**\n",
        "    - region hidden neuron can observe. \n",
        "- only learns the weights connected to that region.\n",
        "- Has shared weights.\n",
        "\n",
        "<img src=\"./image/Hidden_neuron_receptive_field.png\" height=\"250\" />\n",
        "\n",
        "Properties:\n",
        "- **Stride**: pixels between each receptive field pixel\n",
        "- **Padding**: prevent becoming smaller\n",
        "- **Kernel**: receptive field size\n",
        "- **Pooling**: simplify information CNN output; used to reduce the dimensions of the feature maps, thus reducing nr of parameters to learn & computation.\n",
        "    - max pooling: condense information by taking maximum value  Usage:e.g. dark background images\n",
        "    - average pooling: condense information by taking average value Usage: cannot deal with sharp features\n",
        "    - L2 pooling: condense information by taking square root of sum of squares. Usage: smooth features whilst retain property of max pooling\n",
        "\n",
        "Network:\n",
        "- training? net.train\n",
        "- evaluating? net.eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00004-f34d5875-7cdf-420b-830a-8a1dd2949425",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "<img src=\"./image/shallow_cnn_ex.jpeg\" height=\"250\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00002-6a780c9b-a176-4d0f-b931-0ee8ebf92442",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5,
        "execution_start": 1617986370849,
        "source_hash": "60426d5a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================\n",
            "Layer (type:depth-idx)                   Kernel Shape     Input Shape      Output Shape     Param #\n",
            "========================================================================================================\n",
            "├─Conv2d: 1-1                            [3, 5, 5, 5]     [16, 3, 32, 32]  [16, 5, 28, 28]  375\n",
            "├─Conv2d: 1-2                            [5, 2, 3, 3]     [16, 5, 28, 28]  [16, 2, 26, 26]  90\n",
            "========================================================================================================\n",
            "Total params: 465\n",
            "Trainable params: 465\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.35\n",
            "========================================================================================================\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 0.64\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.83\n",
            "========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "batch_size = 16\n",
        "width_image = 32\n",
        "heigh_image = 32\n",
        "layers_image = 3 # Example: 3 -> RGB\n",
        "input_image = (width_image, heigh_image, layers_image)\n",
        "wh_1 = 5 # 1st layer: (Width, Height)\n",
        "f_1 = 5 # 1st layer: nr of filters\n",
        "wh_2 = 3 # 1st layer: (Width, Height)\n",
        "f_2 = 2 # 1st layer: nr of filters\n",
        "bias=False\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(layers_image, f_1, wh_1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(f_1, f_2, wh_2, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv2(self.conv1(x))\n",
        "\n",
        "\n",
        "model_ouput = summary(\n",
        "    Net(), \n",
        "    (batch_size, layers_image, heigh_image, width_image),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"],)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00006-75dfd0a7-28d6-4916-b7cf-d56653f75e19",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "### Example but with stride set to 2.\n",
        "When padding is not added with kernel of size k:\n",
        "- k = odd: k // 2 = px removed each side\n",
        "- k = even: k // 2 = px removed left and top, (k // 2) - 1 = px removed right and bottom (or reverse)\n",
        "Stride (for odd example) is taken into account after previous pixels are removed:\n",
        "- ( height - (2 * (k // 2) )) / stride = new height\n",
        "- ( width - (2 * (k // 2) )) / stride = new width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00006-31a1cb36-a8d1-43ad-b608-2008c7fb6a83",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 43,
        "execution_start": 1618243253721,
        "source_hash": "dcaa10c0",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================\n",
            "Layer (type:depth-idx)                   Kernel Shape     Input Shape      Output Shape     Param #\n",
            "========================================================================================================\n",
            "└─Conv2d: 0-1                            [1, 7, 7, 7]     [16, 1, 226, 226] [16, 7, 110, 110] 343\n",
            "========================================================================================================\n",
            "Total params: 343\n",
            "Trainable params: 343\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 66.40\n",
            "========================================================================================================\n",
            "Input size (MB): 3.27\n",
            "Forward/backward pass size (MB): 10.84\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 14.11\n",
            "========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "batch_size = 16\n",
        "width_image = 226\n",
        "heigh_image = 226\n",
        "layers_image = 1 # Example: 3 -> RGB\n",
        "input_image = (width_image, heigh_image, layers_image)\n",
        "wh_1 = 7 # 1st layer: (Width, Height)\n",
        "f_1 = 7 # 1st layer: nr of filters\n",
        "stride = 2\n",
        "bias=False\n",
        "\n",
        "model_ouput = summary(\n",
        "    nn.Conv2d(layers_image, f_1, wh_1, stride=stride, bias=bias), \n",
        "    (batch_size, layers_image, heigh_image, width_image),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"],)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00008-ae9da3d7-0357-4300-93f1-878a25051207",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "### Example but with stride set to 3.\n",
        "Below we use max pooling, stride 3, causes a decrease of size by 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00008-7a121da3-5495-4f2c-a6a2-0eaf8c04b8e1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 106,
        "execution_start": 1618243237111,
        "source_hash": "6738ff28",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================\n",
            "Layer (type:depth-idx)                   Kernel Shape     Input Shape      Output Shape     Param #\n",
            "========================================================================================================\n",
            "└─MaxPool2d: 0-1                         --               [16, 1, 200, 200] [16, 1, 67, 67]  --\n",
            "========================================================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.00\n",
            "========================================================================================================\n",
            "Input size (MB): 2.56\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 2.56\n",
            "========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "batch_size = 16\n",
        "width_image = 200\n",
        "heigh_image = 200\n",
        "layers_image = 1\n",
        "input_image = (width_image, heigh_image, layers_image)\n",
        "k = 2 # kernel 1st layer: (Width, Height)\n",
        "stride = 3\n",
        "padding = 1 # k // 2\n",
        "bias=False\n",
        "\n",
        "model_ouput = summary(\n",
        "    nn.MaxPool2d((k, k), padding=padding, stride=stride), \n",
        "    (batch_size, layers_image, heigh_image, width_image),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"],)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00010-cd8bd9d3-9ba3-4c80-a504-bdef622b482c",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## Convolution as a matrix multiplication\n",
        "- We make use of a Toeplitz matrix with \\[-1, 1, -1] kernel\n",
        "- We have:\n",
        "    - Kernel of 2\n",
        "    - Padding of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00010-0595f16f-c0e5-4995-956a-5b8ef8e96154",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 40,
        "execution_start": 1618245623028,
        "source_hash": "6c103df",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-35., -27., -53., -62.,  11.]])\n",
            "tensor([[-35., -27.,  11.]])\n"
          ]
        }
      ],
      "source": [
        "input_tensor = torch.tensor([30,20,25,32,60,90,19])\n",
        "\n",
        "conv = torch.tensor([[-1., 0., 0., 0., 0.],\n",
        "        [1., -1., 0., 0., 0.],\n",
        "        [-1., 1., -1., 0., 0.],\n",
        "        [0.,-1., 1., -1., 0.],\n",
        "        [0., 0.,-1., 1., -1.],\n",
        "        [0., 0., 0., -1., 1.],\n",
        "        [0., 0., 0., 0., -1.]])\n",
        "\n",
        "convolve = torch.matmul(input_tensor, conv.long())\n",
        "convolve = convolve.view(1, convolve.shape[0]).float()\n",
        "\n",
        "print(convolve)\n",
        "\n",
        "# Perform max pooling\n",
        "mp = nn.MaxPool1d(2, padding=1)\n",
        "\n",
        "# batch size\n",
        "print(mp(convolve))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "832d570c-1889-433b-b8ca-d922d847299a",
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
