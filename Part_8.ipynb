{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00000-73fa56e6-5fed-4cbe-8e35-5e6e23d1b9b9",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "# Part 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-256c8c7d-edf6-4e04-8681-9c5f8f9c0e98",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## Lecture 8\n",
        "Used for:\n",
        "- Learn compression to store large datasets\n",
        "- pre-training for feature learning\n",
        "- density estimation\n",
        "- initializing weights\n",
        "- generating new data samples\n",
        "\n",
        "**Size of hidden layer**:\n",
        "- Undercomplete \n",
        "    - *h < x*\n",
        "    - Compress the input, good for training samples\n",
        "- Overcomplete \n",
        "    - *h > x*\n",
        "    - No compression, useful for representation learning\n",
        "    - copying input could be prevented with **regularization**\n",
        "\n",
        "<img src=\"./image/over_and_undercomplete.png\" height=\"300\" />\n",
        "\n",
        "- Denoising autoencoder\n",
        "    - Use **regularization** to make robust to noise\n",
        "    - $g(f(x+e)) = x$\n",
        "    - Corrupt the data on purpose \n",
        "- Contractive autoencoder\n",
        "    - Penalize unwanted variations\n",
        "        - if x changes, h does not change much; **robust**\n",
        "    - Frobenius norm of the Jacobian $\\omega(h)$ measures:\n",
        "        - **how much the activations change when input changes**.\n",
        "\n",
        "**Generate samples**?\n",
        "- Auto encoder does not allow for that; is not continuous\n",
        "- Variational allows since it has a hidden layer distribution sampling\n",
        "\n",
        "<img src=\"./image/Encoders.png\" height=\"200\" />\n",
        "\n",
        "**Generative Adversarial Networks**\n",
        "- **Goal**: \n",
        "    - get a discriminator output of > 0.5\n",
        "        - generate new sample images\n",
        "- _sample from high dimensional training distribution_\n",
        "    - add random noise, learn transformation etc.\n",
        "- networks:\n",
        "    - **Generator**: create real looking images\n",
        "        - Aims to minimize D, such that it is close to 1 (for fake data)\n",
        "        - **Only needs random noise as input**\n",
        "    - **Discriminator**: judge if real or fake\n",
        "        - Aims to maximize D, such that it is close to 1 (for real data)\n",
        "- Evaluating likelihoods: Higher likelihood for images does not necessarily mean visually better\n",
        "\n",
        "**VAE** vs **AE**:\n",
        "- Latent space of VAE has all points in latent space close to the origin, yielding meaningful reproductions\n",
        "    - Continuous space, which AE has **not**.\n",
        "- VAE penalizes the **structure of the latent space**\n",
        "- VAE and AE penalize **reconstructions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-a132ca15-6219-4f08-b10f-23d876c432ed",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## [Assignment 8: (Variational) auto-encoders](https://colab.research.google.com/drive/1IbD0o_7XaRQLEt-A3p7cnvCRppV8rkJc)\n",
        "Generating \"meaningful\" samples using unsupervised learning. Unsupervised learners could be used to exploit (hidden) useful structure in data.\n",
        "\n",
        "**Dimension reduction**: \n",
        "- reduce amount of features without losing (most important) information. \n",
        "    <!-- - $$g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k \\quad \\quad n \\gg k$$ -->\n",
        "    - Conversion of *bmp* to *jpeg*.\n",
        "\n",
        "<img src=\"./image/dimension_reduction.png\" height=\"250\" />\n",
        "\n",
        "**The Auto-encoder**:\n",
        "- The outcome ($y =: \\hat{x}$) is compared to the the input so it learns both how to encode the input signals and decode it back. \n",
        "- Is the concatanation of encoder and decoder: $$y = g(h(x)).$$\n",
        "- Loss: $$f_L = L \\left( x, y \\right) = L \\left( x, g(h(x)) \\right).$$\n",
        "- $L$ is mostly chosen to be the MSE-loss.\n",
        "- Assumes not all data is equally important, there is always some noise and it can be reduced to something that only contains the most important information.\n",
        "\n",
        "<img src=\"./image/auto-encoder.png\" height=\"250\" />\n",
        "\n",
        "** Latent space**:\n",
        "- the encoded vector lives in a low dimensional space. \n",
        "    - Often called: **Latent space**\n",
        "    - Potential of generating new data samples\n",
        "- Latent space gaps can cause the generated examples to be \"gibberish\", which happens when you only penalize the outcomes\n",
        "- Variational auto-encoders (VAE's) penalize also the structure of the latent space. This results in generated examples to be less \"gibberish\".\n",
        "\n",
        "<img src=\"./image/auto-encoder-architecture.png\" height=\"250\" />\n",
        "\n",
        "**Architecture**:\n",
        "- to 100 means, limit dimensions to 100 variables\n",
        "- Make use of MSE-loss because this is a regression and not classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00002-2a488ea1-6640-4752-8956-ce1512a61625",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 22,
        "execution_start": 1618052641940,
        "source_hash": "60bbf30d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "Layer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n",
            "========================================================================================================================\n",
            "├─Encoder: 1-1                           [3, 1, 28, 28]   [3, 2]           --               --               --\n",
            "|    └─linear1.weight                                                      [100, 784]\n",
            "|    └─linear2.weight                                                      [50, 100]\n",
            "|    └─linear3.weight                                                      [2, 50]\n",
            "|    └─Linear: 2-1                       [3, 784]         [3, 100]         [784, 100]       78,500           235,200\n",
            "|    └─ReLU: 2-2                         [3, 100]         [3, 100]         --               --               --\n",
            "|    └─Linear: 2-3                       [3, 100]         [3, 50]          [100, 50]        5,050            15,000\n",
            "|    └─ReLU: 2-4                         [3, 50]          [3, 50]          --               --               --\n",
            "|    └─Linear: 2-5                       [3, 50]          [3, 2]           [50, 2]          102              300\n",
            "├─Decoder: 1-2                           [3, 2]           [3, 1, 28, 28]   --               --               --\n",
            "|    └─linear1.weight                                                      [50, 2]\n",
            "|    └─linear2.weight                                                      [100, 50]\n",
            "|    └─linear3.weight                                                      [784, 100]\n",
            "|    └─Linear: 2-6                       [3, 2]           [3, 50]          [2, 50]          150              300\n",
            "|    └─ReLU: 2-7                         [3, 50]          [3, 50]          --               --               --\n",
            "|    └─Linear: 2-8                       [3, 50]          [3, 100]         [50, 100]        5,100            15,000\n",
            "|    └─ReLU: 2-9                         [3, 100]         [3, 100]         --               --               --\n",
            "|    └─Linear: 2-10                      [3, 100]         [3, 784]         [100, 784]       79,184           235,200\n",
            "|    └─Sigmoid: 2-11                     [3, 784]         [3, 784]         --               --               --\n",
            "========================================================================================================================\n",
            "Total params: 168,086\n",
            "Trainable params: 168,086\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 1.25\n",
            "========================================================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.03\n",
            "Params size (MB): 0.67\n",
            "Estimated Total Size (MB): 0.71\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "#encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(s_img*s_img, hdim[0])\n",
        "        self.linear2 = nn.Linear(hdim[0], hdim[1])\n",
        "        self.linear3 = nn.Linear(hdim[1], latent_dims)\n",
        "        self.relu    = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "#decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(latent_dims, hdim[1])\n",
        "        self.linear2 = nn.Linear(hdim[1], hdim[0])\n",
        "        self.linear3 = nn.Linear(hdim[0], s_img*s_img)\n",
        "        self.relu    = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.relu(self.linear1(z))\n",
        "        z = self.relu(self.linear2(z))\n",
        "        z = self.sigmoid(self.linear3(z))\n",
        "        z = z.reshape((-1, 1, s_img, s_img))\n",
        "\n",
        "        return z\n",
        "\n",
        "#autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim = [100, 50]):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, s_img, hdim)\n",
        "        self.decoder = Decoder(latent_dims, s_img, hdim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "# Learnable parameters: \n",
        "n_samples, in_channels, s_img, latent_dims = 3, 1, 28, 2\n",
        "hdim = [100, 50] #choose hidden dimension\n",
        "bias = False\n",
        "\n",
        "model_ouput = summary(\n",
        "    Autoencoder(latent_dims, s_img, hdim),\n",
        "    (n_samples, in_channels, s_img, s_img),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-9b3022bf-b923-4e96-8e06-7486511cdcdd",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "**Variational Auto Encoders**:\n",
        "- Penalize structure and outcome of latent space.\n",
        "- **Reparameterization trick**:\n",
        "    - Used to allow backpropagation\n",
        "    - $$z = \\mu_x +  \\sigma_x \\zeta= g_1(x) + g_2 (x) \\zeta$$ where $\\zeta$ is randomly sampled from $\\mathcal{N} (0, I)$.\n",
        "- _Encoder_: Generates a distribution from which z is chosen randomly. \n",
        "    - Yields a **continuous** and **complete** latent space.\n",
        "    - This is because our encouder outputs a range of possible values from which we'll randomly sample to feed into the decorder model.\n",
        "- _Decoder_: generate new data\n",
        "-  the latent space is regularized if the distribution is penalized.\n",
        "    - the regularization term tries to make the network learn a normal distribution close to mean 0 and variance of 1. \n",
        "    - the reproduction term  is maximizing the reconstruction likelihood\n",
        "    - the regularization term is to encourage learned distribution to be similar to the true prior distribution which we assume to follow Gaussian distribution.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f_{L} &= \\underbrace{L (x, h(z))}_{\\text{reproduction term}} + \\underbrace{R \\left(\\mathcal{N} (\\mu_x, \\sigma_x), \\mathcal{N} (0, I) \\right)}_{\\text{regularization term}} \\\\\n",
        "&= L (x, h(z)) + R \\left(\\mathcal{N} (g_2 (x), g_1(x)), \\mathcal{N} (0, I) \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "<img src=\"./image/var_encoder.png\" height=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00004-f8830535-ff61-4667-8615-df8aac67904c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 22,
        "execution_start": 1618052560956,
        "source_hash": "6c209c0e",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "Layer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n",
            "========================================================================================================================\n",
            "├─VarEncoder: 1-1                        [3, 1, 28, 28]   [3, 2]           --               --               --\n",
            "|    └─linear1_1.weight                                                    [100, 784]\n",
            "|    └─linear2_1.weight                                                    [50, 100]\n",
            "|    └─linear3_1.weight                                                    [2, 50]\n",
            "|    └─linear1_2.weight                                                    [100, 784]\n",
            "|    └─linear2_2.weight                                                    [50, 100]\n",
            "|    └─linear3_2.weight                                                    [2, 50]\n",
            "|    └─Linear: 2-1                       [3, 784]         [3, 100]         [784, 100]       78,500           235,200\n",
            "|    └─ReLU: 2-2                         [3, 100]         [3, 100]         --               --               --\n",
            "|    └─Linear: 2-3                       [3, 100]         [3, 50]          [100, 50]        5,050            15,000\n",
            "|    └─ReLU: 2-4                         [3, 50]          [3, 50]          --               --               --\n",
            "|    └─Linear: 2-5                       [3, 784]         [3, 100]         [784, 100]       78,500           235,200\n",
            "|    └─ReLU: 2-6                         [3, 100]         [3, 100]         --               --               --\n",
            "|    └─Linear: 2-7                       [3, 100]         [3, 50]          [100, 50]        5,050            15,000\n",
            "|    └─ReLU: 2-8                         [3, 50]          [3, 50]          --               --               --\n",
            "|    └─Linear: 2-9                       [3, 50]          [3, 2]           [50, 2]          102              300\n",
            "|    └─Linear: 2-10                      [3, 50]          [3, 2]           [50, 2]          102              300\n",
            "├─Decoder: 1-2                           [3, 2]           [3, 1, 28, 28]   --               --               --\n",
            "|    └─linear1.weight                                                      [50, 2]\n",
            "|    └─linear2.weight                                                      [100, 50]\n",
            "|    └─linear3.weight                                                      [784, 100]\n",
            "|    └─Linear: 2-11                      [3, 2]           [3, 50]          [2, 50]          150              300\n",
            "|    └─ReLU: 2-12                        [3, 50]          [3, 50]          --               --               --\n",
            "|    └─Linear: 2-13                      [3, 50]          [3, 100]         [50, 100]        5,100            15,000\n",
            "|    └─ReLU: 2-14                        [3, 100]         [3, 100]         --               --               --\n",
            "|    └─Linear: 2-15                      [3, 100]         [3, 784]         [100, 784]       79,184           235,200\n",
            "|    └─Sigmoid: 2-16                     [3, 784]         [3, 784]         --               --               --\n",
            "========================================================================================================================\n",
            "Total params: 251,738\n",
            "Trainable params: 251,738\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 2.00\n",
            "========================================================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.03\n",
            "Params size (MB): 1.01\n",
            "Estimated Total Size (MB): 1.05\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "#encoder\n",
        "class VarEncoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(VarEncoder, self).__init__()\n",
        "        \n",
        "        #layers for g1\n",
        "        self.linear1_1 = nn.Linear(s_img*s_img, hdim[0])\n",
        "        self.linear2_1 = nn.Linear(hdim[0], hdim[1])\n",
        "        self.linear3_1 = nn.Linear(hdim[1], latent_dims)\n",
        "\n",
        "        #layers for g2\n",
        "        self.linear1_2 = nn.Linear(s_img*s_img, hdim[0])\n",
        "        self.linear2_2 = nn.Linear(hdim[0], hdim[1])\n",
        "        self.linear3_2 = nn.Linear(hdim[1], latent_dims)\n",
        "\n",
        "        self.relu    = nn.ReLU()\n",
        "\n",
        "        #distribution setup\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc\n",
        "        self.N.scale = self.N.scale\n",
        "        self.kl = 0\n",
        "\n",
        "    def kull_leib(self, mu, sigma):\n",
        "        return (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
        "\n",
        "    def reparameterize(self, mu, sig):\n",
        "        return mu + sig*self.N.sample(mu.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        \n",
        "        x1 = self.relu(self.linear1_1(x))\n",
        "        x1 = self.relu(self.linear2_1(x1))\n",
        "\n",
        "        x2 = self.relu(self.linear1_2(x))\n",
        "        x2 = self.relu(self.linear2_2(x2))\n",
        "\n",
        "        sig = torch.exp(self.linear3_1(x1))\n",
        "        mu = self.linear3_2(x2)\n",
        "\n",
        "        #reparameterize to find z\n",
        "        z = self.reparameterize(mu, sig)\n",
        "\n",
        "        #loss between N(0,I) and learned distribution\n",
        "        self.kl = self.kull_leib(mu, sig)\n",
        "\n",
        "        return z\n",
        "\n",
        "#decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(latent_dims, hdim[1])\n",
        "        self.linear2 = nn.Linear(hdim[1], hdim[0])\n",
        "        self.linear3 = nn.Linear(hdim[0], s_img*s_img)\n",
        "        self.relu    = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.relu(self.linear1(z))\n",
        "        z = self.relu(self.linear2(z))\n",
        "        z = self.sigmoid(self.linear3(z))\n",
        "        z = z.reshape((-1, 1, s_img, s_img))\n",
        "\n",
        "        return z\n",
        "\n",
        "#autoencoder\n",
        "class VarAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim = [100, 50]):\n",
        "        super(VarAutoencoder, self).__init__()\n",
        "        self.encoder = VarEncoder(latent_dims, s_img, hdim)\n",
        "        self.decoder = Decoder(latent_dims, s_img, hdim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "# Learnable parameters: \n",
        "n_samples, in_channels, s_img, latent_dims = 3, 1, 28, 2\n",
        "hdim = [100, 50] #choose hidden dimension\n",
        "bias = False\n",
        "\n",
        "model_ouput = summary(\n",
        "    VarAutoencoder(latent_dims, s_img, hdim),\n",
        "    (n_samples, in_channels, s_img, s_img),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "a45e7391-cf1c-46c6-9fab-a697156c90d2",
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
