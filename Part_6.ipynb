{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00000-9fd6ee2f-b039-4f0f-a5e3-aae0dc4c3526",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "# Part 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-8c3c3992-cf04-4968-8281-c5ad352ed422",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## Lecture 6\n",
        "<img src=\"./image/types_of_sequential_data.png\" height=\"200\" />\n",
        "\n",
        "- _FFN_ is not used since it will learn exact word locations.\n",
        "- _RNN_ alleviates by using **shared weights**.\n",
        "    - It is recurrent to allow for sequential order processing\n",
        "    - concatenate: $A \\cdot B + C \\cdot D = A|C \\cdot \\frac{B}{D}$ (where lines are concatenation)\n",
        "    - Cannot be trained in parallel, since its sequential.\n",
        "    - **Teacher forcing** can be used to train in *Parallel*\n",
        "        - cuts dependency with previous time-step\n",
        "        - **not** as powerful as RNN\n",
        "    - Gradient of sequence length for 1000+ (arbitrary big number)\n",
        "        - \\> 1, it explodes\n",
        "        - < 1, it vanishes\n",
        "        - This happens because of text having long-range relations/references.\n",
        "\n",
        "<img src=\"./image/Simple_RNN.png\" height=\"200\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-b43dd411-ee42-437e-817f-51e249244ad1",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## [Assignment 6: RNN, GRU and LSTM](https://colab.research.google.com/drive/1Y71ysVaEHOw0lPRqDGOL-OmTLHB0X-HI)\n",
        "- Deal with variable length sequential data\n",
        "\n",
        "**RNN**\n",
        "- recursively update states via the forward pass.\n",
        "- recurrences depend on length of sequence.\n",
        "- Nr of hidden states is preserved, irregardless of the sequence length.\n",
        "    - (Hidden) state: $H_t = φ(X_tW_{xh} + H_{t-1}W_{hh} + b_h)$\n",
        "- x will have same size at every iteration\n",
        "- Tanh is often chosen as function for RNN\n",
        "- Also called Vanilla or Elman RNN\n",
        "    - Difficult to deal with **long term dependencies** compared to GRU and LSTM\n",
        "\n",
        "<img src=\"./image/Example_language_model.png\" height=\"350\" />\n",
        "\n",
        "- **in** = (seq_length, n_timesteps, input_size)\n",
        "- RNN layer = (input_size, hidden_size)\n",
        "    - Weight_xh = (input_size, hidden_size)\n",
        "    - Weight_hh = (hidden_size, hidden_size)\n",
        "    - Bias_xh = (hidden_size)\n",
        "    - Bias_hh = (hidden_size)\n",
        "- **out** = (seq_length, n_timesteps, hidden_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00003-be4c50a5-dbe1-42de-aff2-260335c0c0b5",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 7,
        "execution_start": 1617992338874,
        "source_hash": "cd5ddb44",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "Layer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n",
            "========================================================================================================================\n",
            "└─RNN: 0-1                               [2, 3, 4]        [2, 3, 10]       --               160              140\n",
            "├─weight_ih_l0                                                             [10, 4]\n",
            "├─weight_hh_l0                                                             [10, 10]\n",
            "========================================================================================================================\n",
            "Total params: 160\n",
            "Trainable params: 160\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.00\n",
            "========================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Vanilla RNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "seq_length, n_timesteps, input_size, hidden_size = 2, 3, 4, 10\n",
        "bias = True\n",
        "bidirectional = False\n",
        "\n",
        "# total_param = 1 * ( input_size + hidden_size + ( 2 <- bias ) ) * hidden_size ( * 2 if bi-directional )\n",
        "RNN = nn.RNN(\n",
        "    input_size=input_size, \n",
        "    hidden_size=hidden_size, \n",
        "    num_layers=1, \n",
        "    bias=bias, \n",
        "    batch_first=True, \n",
        "    bidirectional=bidirectional\n",
        "    )\n",
        "\n",
        "model_ouput = summary(\n",
        "    RNN,\n",
        "    (seq_length, n_timesteps, input_size),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00003-b299c3ac-0ff8-4136-b432-97a651c5ee01",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "**GRU**\n",
        "- Gated Recurrent Unit\n",
        "    - Gating allows for long term information to pass through unchanged.\n",
        "- ($R_t$) **Reset gate**: Learn what info must be forgotten or kept \n",
        "    - Reset for different logical chunks in input\n",
        "    - Ex. *chapters in a book*\n",
        "- ($Z_t$) **Update gate**: Learn what new info must be added to the current hidden state\n",
        "    - Don't update for uninformative input \n",
        "    - Ex. *html encodings*\n",
        "\n",
        "<img src=\"./image/GRU.png\" height=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00004-a061e622-81aa-47bb-ac06-fc87893c289f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 9,
        "execution_start": 1617992333605,
        "source_hash": "1a484474",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "Layer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n",
            "========================================================================================================================\n",
            "└─GRU: 0-1                               [2, 3, 4]        [2, 3, 10]       --               420              420\n",
            "├─weight_ih_l0                                                             [30, 4]\n",
            "├─weight_hh_l0                                                             [30, 10]\n",
            "========================================================================================================================\n",
            "Total params: 420\n",
            "Trainable params: 420\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.00\n",
            "========================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# GRU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "seq_length, n_timesteps, input_size, hidden_size = 2, 3, 4, 10\n",
        "bias = False\n",
        "bidirectional = False\n",
        "\n",
        "# total_param = 3 * ( input_size + hidden_size + ( 2 <- bias ) ) * hidden_size ( * 2 if bi-directional )\n",
        "GRU = nn.GRU(\n",
        "    input_size=input_size, \n",
        "    hidden_size=hidden_size, \n",
        "    num_layers=1, \n",
        "    bias=bias, \n",
        "    batch_first=True, \n",
        "    bidirectional=bidirectional\n",
        "    )\n",
        "\n",
        "model_ouput = summary(\n",
        "    GRU,\n",
        "    (seq_length, n_timesteps, input_size),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00005-6126fc6d-a374-46b3-b1af-eafe1b6bc938",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "**LSTM**\n",
        "- Long Short-term Memories\n",
        "- ($I_t$) **Input gate**:\n",
        "- ($F_t$) **Forget gate**: Learn what need to be forgotten or kept\n",
        "- ($O_t$) **Output gate**:\n",
        "- ($C_t$) **Cell state**: keep track of the state of the cell, what old info should be forgotten, pushed through and what new info added.\n",
        "    - Combine forget and update -> updated cell state\n",
        "- Sigmoid used to calculate which values to remember and forget (\\[0, 1])\n",
        "- Memory = cell state\n",
        "\n",
        "<img src=\"./image/LSTM.png\" height=\"300\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00005-b5f3d3a2-77ab-4a8c-8ea2-025262971286",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 667,
        "execution_start": 1618040466734,
        "source_hash": "b6fbc793",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "Layer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n",
            "========================================================================================================================\n",
            "└─LSTM: 0-1                              [2, 3, 4]        [2, 3, 10]       --               560              3,360\n",
            "├─weight_ih_l0                                                             [40, 4]\n",
            "├─weight_hh_l0                                                             [40, 10]\n",
            "========================================================================================================================\n",
            "Total params: 560\n",
            "Trainable params: 560\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.00\n",
            "========================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "seq_length, n_timesteps, input_size, hidden_size = 2, 3, 4, 10\n",
        "bias = False\n",
        "bidirectional = False\n",
        "\n",
        "# total_param = 4 * ( input_size + hidden_size + ( 2 <- bias ) ) * hidden_size ( * 2 if bi-directional )\n",
        "LSTM = nn.LSTM(\n",
        "    input_size=input_size, \n",
        "    hidden_size=hidden_size, \n",
        "    num_layers=1, \n",
        "    bias=bias, \n",
        "    batch_first=True, \n",
        "    bidirectional=bidirectional\n",
        "    )\n",
        "\n",
        "model_ouput = summary(\n",
        "    LSTM,\n",
        "    (seq_length, n_timesteps, input_size),\n",
        "    verbose=2,\n",
        "    col_width=16,\n",
        "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "66376b6e-4068-4fb7-9bb3-589254d514b9",
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
