{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Part 2",
      "metadata": {
        "tags": [],
        "cell_id": "00000-88b2f020-961b-43ed-991b-8d96ce8de60d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Lecture 2\n\n**Maximum likelihood**:\n- Estimate parameters that is most likely\n- Assumption:\n    - drawn from i.i.d. from unknown distribution\n- Problem:\n    - Diminishes when multiplying probabilities (numerically unstable)\n    - Use log (turn multiplication into sum)\n- Same as:\n    - MLE: maximize\n    - KL: minimize (related to cross-entropy between two distributions; MSE is cross entropy between empirical distribution and a Gaussian)\n\n**Sigmoid**:\n- Loss wants to be exact\n    - Ex: Use square loss (without Sigmoid) -> when doing classification, correct classifications should give high loss\n        - Thus use Sigmoid, squash between \\[0, 1]\n    - Ex: Use bernoulli cross-entropy loss instead of square loss -> penalize small difference of really wrong predictions (thus bigger steps when far from optimal.)\n\n**Logistic regression**:\n- Use Sigmoid + Bernoulli cross entropy loss: Penalize very wrong predictions (two class)\n- Softmax is generalization of logisitic function (multiclass)\n\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00001-8776eade-3dc3-4149-a9eb-04ddcdf81603",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Assignment 2: Backward propagation\nComputing the partial derivates of a loss function with respect to the parameters of a network. It tries to **share** the repeated computations wherever possible. ***Forward pass*** gets the loss, ***Backward pass*** computes derivatives to adjust the parameters.\n- ex. $f(g(x, y))$, when using the chain-rule, the results of $f()$ are re-used.\n\n<!-- <img src=\"/image/compute_graph.png\" height=\"200\" />\n\n- Derivative of *w*: $$\\frac{d\\mathcal{L}}{dw} = \\frac{dL}{dy}\\frac{dy}{dz}\\frac{dz}{dw}$$\n- Derivative of *b*: $$\\frac{d\\mathcal{L}}{db} = \\frac{dL}{dy}\\frac{dy}{dz}\\frac{dz}{db}$$\n\nCould be more efficient by cutting into 4:\n- $$\\frac{d\\mathcal{L}}{dy} = y - t$$\n- $$\\frac{d\\mathcal{L}}{dz} = \\frac{d\\mathcal{L}}{dy}\\sigma'(z)$$\n- $$\\frac{d\\mathcal{L}}{dw} = \\frac{d\\mathcal{L}}{dz}x$$\n- $$\\frac{d\\mathcal{L}}{db} = \\frac{d\\mathcal{L}}{dz}$$\n\n<img src=\"/image/backprop.png\" height=\"200\" /> -->",
      "metadata": {
        "tags": [],
        "cell_id": "00001-bfe67196-b693-4ea2-8e3f-7c56fc6491bc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Non-linear Functions:\n- $$\\text{softmax}(\\mathbf{z})_k = \\frac{\\exp{z_k}}{\\sum_i^K \\exp{z_i}},$$\n    - interpret output as class probabilities, bounded \\[0, 1]\n    - Sigmoid extension -> saturation problem (learning slow down)\n    - Overcome by combining CE with Softmax (because of log)\n\nLoss Functions:\n- $$\\text{CE} = H(y,p) = -\\sum_i^K y_i \\log(p_i),$$\n    - Log allows for stability for computing small values\n- Squared error\n    - Too high values for predictions with high confidence.\n- KL divergence",
      "metadata": {
        "tags": [],
        "cell_id": "00002-f608b124-483a-45fd-8861-f423818a9dba",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "93f6a13a-6648-4e60-b2da-b562da2ae4bb",
    "deepnote_execution_queue": []
  }
}